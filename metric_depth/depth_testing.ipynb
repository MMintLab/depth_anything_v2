{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_image_grid(images, captions, path, ncol=7, normalize = True, save = True):\n",
    "    if not normalize:\n",
    "        norm_text = \"_not_normalized\"\n",
    "    else:\n",
    "        norm_text = \"\"\n",
    "\n",
    "    grids = [make_grid(img, nrow=ncol,padding=1, normalize=normalize, scale_each=True) for img in images]\n",
    "    for grid, caption in zip(grids, captions):\n",
    "        if save:\n",
    "            save_image(grid, path +  '/' + caption + norm_text + '.png')\n",
    "        else:\n",
    "            plt.imshow(np.asarray(grid.permute((1,2,0)).cpu()[:,:,0]), cmap='plasma')\n",
    "            plt.title(caption)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "gt_path = \"/home/samanta/touch2touch/results/ground_truth\"\n",
    "models_path = \"/home/samanta/depth_anything_v2/metric_depth/results\"\n",
    "models_paths = ['fine_tuning_bubbles_max0.12_no_mask_not_zero_train_tools_improved_data']\n",
    "models_paths = [os.path.join(models_path, path) for path in models_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine_tuning_bubbles_max0.12_no_mask_not_zero_train_tools_improved_data\n",
      "test\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 3 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m gt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(gt, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m gt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m     logging_image_grid([inputs, gt], [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGT_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGT_PCD_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ncol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 3 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "for i, model_path in enumerate(models_paths):\n",
    "    model = model_path.split('/')[-1]\n",
    "    print(model)\n",
    "    datasets_paths = os.listdir(model_path)\n",
    "    datasets_paths = [path for path in datasets_paths if os.path.isdir(os.path.join(model_path, path))]\n",
    "    datasets_paths.sort()\n",
    "    for dataset in datasets_paths:\n",
    "        inputs = []\n",
    "        gt = []\n",
    "        pred = []\n",
    "        dataset_path = os.path.join(model_path, dataset)\n",
    "        datasets_paths = ['test_unseen']\n",
    "        print(dataset)\n",
    "        tools_paths = os.listdir(dataset_path)\n",
    "        tools_paths.sort()\n",
    "        tools_paths = tools_paths[:4]\n",
    "        for tool in tools_paths:\n",
    "            tool_path = os.path.join(dataset_path, tool)\n",
    "            tool_input = torch.load(os.path.join(tool_path, \"depth_qualitative_results.pt\"))['bubbles_img_viz_single']\n",
    "            tool_gt = torch.load(os.path.join(tool_path, \"depth_qualitative_results.pt\"))['depth_gt_viz_single']\n",
    "            tool_pred = torch.load(os.path.join(tool_path, \"depth_qualitative_results.pt\"))['depth_pred_viz_single']\n",
    "            inputs.append(tool_input)\n",
    "            gt.append(tool_gt)\n",
    "            pred.append(tool_pred)\n",
    "\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "        gt = torch.cat(gt, dim=0)\n",
    "        pred = torch.cat(pred, dim=0)\n",
    "        gt = torch.cat([inputs, gt, pred], dim=0)\n",
    "        \n",
    "\n",
    "        if i == 0:\n",
    "            logging_image_grid([inputs, gt], [f\"Input_{dataset}\", f\"GT_{dataset}\", f\"GT_PCD_{dataset}\"], '',save=False, ncol=4)\n",
    "        logging_image_grid([pred], [f\"Output_{dataset}\", f\"Output_PCD_{model}_{dataset}\"], \"\", save=False, ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Quantitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Results:\n",
      "Empty DataFrame\n",
      "Columns: [method, dataset, AbsRel, RMSE, LogRMSE, SiLog]\n",
      "Index: []\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Results:\n",
      "Empty DataFrame\n",
      "Columns: [method, dataset, AbsRel, RMSE, LogRMSE, SiLog]\n",
      "Index: []\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Test_unseen Results:\n",
      "                                              method      dataset  AbsRel  \\\n",
      "0  fine_tuning_bubbles_max0.12_no_mask_not_zero_t...  test_unseen  0.1135   \n",
      "\n",
      "     RMSE  LogRMSE   SiLog  \n",
      "0  0.0236   3.3096  2.7207  \n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_results = []\n",
    "\n",
    "for i, model_path in enumerate(models_paths):\n",
    "    model = model_path.split('/')[-1]\n",
    "    datasets_paths = ['test_unseen']\n",
    "    datasets_paths = [path for path in datasets_paths if os.path.isdir(os.path.join(model_path, path))]\n",
    "    datasets_paths.sort()\n",
    "    \n",
    "    for dataset in datasets_paths:\n",
    "        abs_rel_error = 0\n",
    "        rmse_error = 0\n",
    "        log_rmse_error = 0\n",
    "        silog_error = 0\n",
    "\n",
    "        dataset_path = os.path.join(model_path, dataset)\n",
    "        tools_paths = os.listdir(dataset_path)\n",
    "        tools_paths.sort()\n",
    "        # tools_paths = tools_paths[:4]\n",
    "\n",
    "        for tool in tools_paths:\n",
    "            tool_path = os.path.join(dataset_path, tool)\n",
    "            metrics = torch.load(os.path.join(tool_path, \"depth_quantitative_results.pt\"))\n",
    "\n",
    "            abs_rel_error += metrics['abs_rel']\n",
    "            rmse_error += metrics['rmse']\n",
    "            log_rmse_error += metrics['log_rmse']\n",
    "            silog_error += metrics['silog']\n",
    "\n",
    "        # Compute average errors over all tools\n",
    "        abs_rel_error /= len(tools_paths)\n",
    "        rmse_error /= len(tools_paths)\n",
    "        log_rmse_error /= len(tools_paths)\n",
    "        silog_error /= len(tools_paths)\n",
    "        \n",
    "        # Store results in dictionary\n",
    "        metrics_dict = {\n",
    "            'method': model, \n",
    "            'dataset': dataset, \n",
    "            'AbsRel': abs_rel_error, \n",
    "            'RMSE': rmse_error, \n",
    "            'LogRMSE': log_rmse_error, \n",
    "            'SiLog': silog_error\n",
    "        }\n",
    "        \n",
    "        metrics_results.append(metrics_dict)\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "\n",
    "# Print results for different datasets\n",
    "for dataset_name in ['train', 'test', 'test_unseen']:\n",
    "    filtered_df = metrics_df[metrics_df['dataset'] == dataset_name]\n",
    "    pd.set_option('display.precision', 4)  # Set precision for better readability\n",
    "    print(f'{dataset_name.capitalize()} Results:')\n",
    "    print(filtered_df)\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth_anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
